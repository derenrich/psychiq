{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMG29SO9A7VP"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'wikidata-319717'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUmua28057Cx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyDGJE0bBq3R"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets\n",
        "!pip install evaluate \n",
        "!pip install huggingface_hub scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvgP6ns4odpv"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://wikidata-de/relation_ids.parquet ."
      ],
      "metadata": {
        "id": "wU8rKVGnaA0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzqoaC-hBjax"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import transformers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/gdrive/MyDrive/psychiq/model3/df.parquet\")"
      ],
      "metadata": {
        "id": "aCIfoHPikyA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw56NElgDNk_"
      },
      "outputs": [],
      "source": [
        "relations = pd.read_parquet('relation_ids.parquet')\n",
        "# map from 0 to 999\n",
        "relations.relation_id -= 1\n",
        "relation_map = dict()\n",
        "for i, row in relations.iterrows():\n",
        "  relation_map[row['relation_id']] = (row['relation'], row['target'])\n",
        "relation_map[1000] = ('unknown', 'unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li4Lmr4NB89m"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Features, ClassLabel, Value\n",
        "names = [f\"{p}-{qid}\" for (_, (p,qid)) in sorted(relation_map.items())]\n",
        "id2name = dict(list(enumerate(names)))\n",
        "name2id = {v: k for k, v in id2name.items()}\n",
        "features = Features({'text': Value('string'), 'labels': ClassLabel(1001, names=names)})"
      ],
      "metadata": {
        "id": "S4hHA-shaLcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "tokenized_training = load_from_disk('/content/gdrive/MyDrive/psychiq/model3/split_dataset')"
      ],
      "metadata": {
        "id": "7GOqX1buYfC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmdKtCqVCMwJ"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\",label2id=name2id, id2label=id2name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/gdrive/MyDrive/psychiq/model3\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_steps =10000,\n",
        "    save_total_limit=2,\n",
        "    hub_model_id=\"derenrich/psychiq2\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_training['train'],\n",
        "    eval_dataset=tokenized_training['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train(resume_from_checkpoint = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIJfPag5DOkF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "\n",
        "test = load_from_disk(\"/content/gdrive/MyDrive/psychiq/model3/split_test\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/gdrive/MyDrive/psychiq/model3\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_steps =10000,\n",
        "    save_total_limit=2,\n",
        "    hub_model_id=\"derenrich/psychiq2\"\n",
        ")\n",
        "\n",
        "NSHARDS = 10\n",
        "for i in range(NSHARDS):\n",
        "  train = load_from_disk(\"/content/gdrive/MyDrive/psychiq/model3/split_train_\" + (str(i)))\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train,\n",
        "      eval_dataset=test,\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "  )\n",
        "  trainer.train(resume_from_checkpoint = (i!=0))\n",
        "  del train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pd4o9obDQh9"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(tokenized_training['test'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(tokenized_training['test'])"
      ],
      "metadata": {
        "id": "lr0GRBSWh1jL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}